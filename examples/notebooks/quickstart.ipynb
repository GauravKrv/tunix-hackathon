{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLHF Training Quickstart Guide\n",
    "\n",
    "This notebook demonstrates an end-to-end workflow for training and inference with reinforcement learning from human feedback (RLHF).\n",
    "\n",
    "## Overview\n",
    "\n",
    "We'll cover:\n",
    "1. Environment setup\n",
    "2. Loading and preparing data\n",
    "3. Model configuration\n",
    "4. Reward model setup\n",
    "5. Training with PPO\n",
    "6. Evaluation\n",
    "7. Inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, install the required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install torch transformers accelerate peft datasets trl bitsandbytes\n",
    "# !pip install tensorboard wandb\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Configuration\n",
    "\n",
    "We'll start with a basic training recipe for Gemma3 1B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration from recipe\n",
    "def load_config(config_path):\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "\n",
    "# For this demo, we'll use the Gemma3 1B basic recipe\n",
    "config_path = '../recipes/gemma3_1b_basic.yaml'\n",
    "\n",
    "# Load and display config\n",
    "config = load_config(config_path)\n",
    "print(\"Configuration loaded:\")\n",
    "print(yaml.dump(config, default_flow_style=False, indent=2)[:500] + '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Preparation\n",
    "\n",
    "Load and preprocess the dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "model_name = \"google/gemma-3-1b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Ensure padding token is set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Tokenizer loaded: {model_name}\")\n",
    "print(f\"Vocab size: {len(tokenizer)}\")\n",
    "print(f\"Padding token: {tokenizer.pad_token}\")\n",
    "\n",
    "# Load dataset (using a small subset for demo)\n",
    "print(\"\\nLoading dataset...\")\n",
    "dataset = load_dataset(\"Anthropic/hh-rlhf\", split=\"train[:100]\")\n",
    "print(f\"Dataset loaded: {len(dataset)} samples\")\n",
    "print(f\"\\nExample prompt:\\n{dataset[0]['chosen'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Loading\n",
    "\n",
    "Load the base model with LoRA for efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Load base model\n",
    "print(\"Loading base model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"\\nModel with LoRA:\")\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Enable gradient checkpointing for memory efficiency\n",
    "model.gradient_checkpointing_enable()\n",
    "model.config.use_cache = False\n",
    "\n",
    "print(\"\\nModel loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Reward Model Setup\n",
    "\n",
    "Load the reward model for scoring generated responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load reward model\n",
    "print(\"Loading reward model...\")\n",
    "reward_model_name = \"OpenAssistant/reward-model-deberta-v3-large-v2\"\n",
    "\n",
    "reward_pipe = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=reward_model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "print(f\"Reward model loaded: {reward_model_name}\")\n",
    "\n",
    "# Test reward model\n",
    "test_text = \"This is a helpful and informative response to the user's question.\"\n",
    "reward_score = reward_pipe(test_text)[0]['score']\n",
    "print(f\"\\nTest reward score: {reward_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Configuration\n",
    "\n",
    "Set up the PPO trainer with appropriate hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead\n",
    "from trl.core import LengthSampler\n",
    "\n",
    "# Create model with value head for PPO\n",
    "print(\"Creating model with value head...\")\n",
    "ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(model)\n",
    "\n",
    "# PPO configuration\n",
    "ppo_config = PPOConfig(\n",
    "    model_name=model_name,\n",
    "    learning_rate=2e-5,\n",
    "    batch_size=4,\n",
    "    mini_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    ppo_epochs=4,\n",
    "    early_stopping=False,\n",
    "    target_kl=0.1,\n",
    "    kl_penalty=\"kl\",\n",
    "    seed=42,\n",
    "    log_with=\"tensorboard\",\n",
    "    project_kwargs={\"logging_dir\": \"./logs/quickstart\"}\n",
    ")\n",
    "\n",
    "print(\"PPO configuration created:\")\n",
    "print(f\"  Learning rate: {ppo_config.learning_rate}\")\n",
    "print(f\"  Batch size: {ppo_config.batch_size}\")\n",
    "print(f\"  Mini batch size: {ppo_config.mini_batch_size}\")\n",
    "print(f\"  PPO epochs: {ppo_config.ppo_epochs}\")\n",
    "print(f\"  Target KL: {ppo_config.target_kl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Loop\n",
    "\n",
    "Run the PPO training loop (simplified for demo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config=ppo_config,\n",
    "    model=ppo_model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=dataset\n",
    ")\n",
    "\n",
    "# Generation settings\n",
    "generation_kwargs = {\n",
    "    \"max_new_tokens\": 128,\n",
    "    \"temperature\": 1.0,\n",
    "    \"top_p\": 0.9,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id\n",
    "}\n",
    "\n",
    "print(\"\\nStarting training...\\n\")\n",
    "print(\"Note: This is a simplified demo. For full training, use the main training script.\")\n",
    "\n",
    "# Training loop (shortened for demo)\n",
    "num_steps = 10\n",
    "\n",
    "for step, batch in enumerate(ppo_trainer.dataloader):\n",
    "    if step >= num_steps:\n",
    "        break\n",
    "    \n",
    "    # Extract prompts\n",
    "    query_tensors = batch[\"input_ids\"]\n",
    "    \n",
    "    # Generate responses\n",
    "    response_tensors = ppo_trainer.generate(\n",
    "        query_tensors,\n",
    "        return_prompt=False,\n",
    "        **generation_kwargs\n",
    "    )\n",
    "    \n",
    "    # Decode responses\n",
    "    batch[\"response\"] = tokenizer.batch_decode(response_tensors, skip_special_tokens=True)\n",
    "    \n",
    "    # Compute rewards\n",
    "    texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "    rewards = [torch.tensor(reward_pipe(text)[0]['score']) for text in texts]\n",
    "    \n",
    "    # Run PPO step\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    \n",
    "    # Log progress\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)\n",
    "    \n",
    "    if step % 5 == 0:\n",
    "        print(f\"Step {step}: Mean reward = {torch.stack(rewards).mean():.4f}\")\n",
    "\n",
    "print(\"\\nTraining demo complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Model\n",
    "\n",
    "Save the trained model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save directory\n",
    "save_dir = \"./checkpoints/quickstart_demo\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "ppo_trainer.save_pretrained(save_dir)\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Inference\n",
    "\n",
    "Load the trained model and run inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load trained model\n",
    "print(\"Loading trained model...\")\n",
    "trained_model = AutoModelForCausalLM.from_pretrained(\n",
    "    save_dir,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "trained_tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Inference function\n",
    "def generate_response(prompt, max_new_tokens=256, temperature=0.7, top_p=0.9):\n",
    "    inputs = trained_tokenizer(prompt, return_tensors=\"pt\").to(trained_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = trained_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=trained_tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    response = trained_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"What is machine learning?\",\n",
    "    \"How can I improve my productivity?\",\n",
    "    \"Explain quantum computing in simple terms.\"\n",
    "]\n",
    "\n",
    "print(\"\\nGenerating responses...\\n\")\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Prompt {i}: {prompt}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    response = generate_response(prompt)\n",
    "    print(f\"Response:\\n{response}\")\n",
    "    \n",
    "    # Compute reward for the response\n",
    "    reward = reward_pipe(response)[0]['score']\n",
    "    print(f\"\\nReward score: {reward:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Compare with Base Model\n",
    "\n",
    "Compare outputs from the base model and fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model for comparison\n",
    "print(\"Loading base model for comparison...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "def generate_base_response(prompt, max_new_tokens=256):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(base_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = base_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Compare responses\n",
    "prompt = \"What are the benefits of regular exercise?\"\n",
    "\n",
    "print(f\"\\nPrompt: {prompt}\\n\")\n",
    "\n",
    "print(\"Base Model Response:\")\n",
    "base_response = generate_base_response(prompt)\n",
    "print(base_response)\n",
    "base_reward = reward_pipe(base_response)[0]['score']\n",
    "print(f\"Reward: {base_reward:.4f}\\n\")\n",
    "\n",
    "print(\"Fine-tuned Model Response:\")\n",
    "tuned_response = generate_response(prompt)\n",
    "print(tuned_response)\n",
    "tuned_reward = reward_pipe(tuned_response)[0]['score']\n",
    "print(f\"Reward: {tuned_reward:.4f}\\n\")\n",
    "\n",
    "print(f\"Reward improvement: {tuned_reward - base_reward:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Next Steps\n",
    "\n",
    "This quickstart covered the basics of RLHF training. To dive deeper:\n",
    "\n",
    "### Full Training\n",
    "For production training, use the command-line interface:\n",
    "```bash\n",
    "python train.py --config examples/recipes/gemma3_1b_basic.yaml\n",
    "```\n",
    "\n",
    "### Experiment with Different Configurations\n",
    "- Try different model sizes: `gemma2_2b` vs `gemma3_1b`\n",
    "- Use different datasets: `anthropic_hh`, `openassistant`, `summarization`\n",
    "- Experiment with reward compositions: `single_reward`, `multi_objective`, `ensemble`\n",
    "\n",
    "### Advanced Features\n",
    "- Multi-GPU training for faster convergence\n",
    "- Experiment tracking with Weights & Biases\n",
    "- Custom reward functions\n",
    "- Hyperparameter tuning\n",
    "\n",
    "### Evaluation\n",
    "- Run comprehensive evaluations on held-out test sets\n",
    "- Compare multiple checkpoints\n",
    "- Analyze failure modes and edge cases\n",
    "\n",
    "### Resources\n",
    "- Documentation: `docs/`\n",
    "- Example configs: `examples/configs/`\n",
    "- Training recipes: `examples/recipes/`\n",
    "- Community examples: `examples/community/`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this quickstart, we:\n",
    "1. ✅ Set up the environment and dependencies\n",
    "2. ✅ Loaded a configuration and dataset\n",
    "3. ✅ Initialized a model with LoRA for efficient training\n",
    "4. ✅ Set up a reward model for scoring responses\n",
    "5. ✅ Ran a simplified PPO training loop\n",
    "6. ✅ Saved the trained model\n",
    "7. ✅ Performed inference with the fine-tuned model\n",
    "8. ✅ Compared base and fine-tuned model outputs\n",
    "\n",
    "You're now ready to explore more advanced RLHF training scenarios!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
